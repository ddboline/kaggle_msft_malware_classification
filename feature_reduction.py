#!/usr/bin/python

import os
import gzip
import csv
import glob
import multiprocessing
from collections import deque, defaultdict

import pandas as pd
import numpy as np

def read_feature_dict(tclasses, tdict, inq, outq):
    per_class_ngram_count = {cl: defaultdict(int) for cl in tclasses}
    while True:
        if inq.empty():
            continue
        prefix = inq.get()
        if prefix == 'STOP':
            outq.put(per_class_ngram_count)
            break
        else:
            pkey = os.path.basename(prefix)
            class_ = train_dict[pkey]
            outq.put(prefix)
            with gzip.open('%s.ngram.gz' % prefix, 'rb') as ngramfile:
                for line in ngramfile:
                    if line.find('ngram') == 0:
                        continue
                    w, c = line.split(',')
                    per_class_ngram_count[class_][w] += int(c)
    return

if __name__ == '__main__':
    train_labels = pd.read_csv('trainLabels.csv')
    classes = set(train_labels['Class'].unique())
    train_dict = dict(zip(train_labels['Id'].tolist(), train_labels['Class'].tolist()))
    dir_to_process = os.sys.argv[1]
    ncpu = len(filter(lambda x: x.find('processor')==0, 
                      open('/proc/cpuinfo')
                      .read().split('\n')))
    print 'ncpu', ncpu
    prefixes = map(lambda x: x.replace('.ngram.gz',''), glob.glob('%s/*.ngram.gz' % dir_to_process))
    total_ngram_count = defaultdict(int)
    per_class_ngram_count = {cl: defaultdict(int) for cl in classes}
    
    input_queue = multiprocessing.Queue()
    output_queue = multiprocessing.Queue()
    
    workers = [multiprocessing.Process(target=read_feature_dict, args=(classes, train_dict, input_queue, output_queue,)) for _ in range(ncpu)]
    
    for w in workers:
        w.start()
    
    for prefix in prefixes:
        input_queue.put(prefix)
    
    for w in workers:
        input_queue.put('STOP')
    
    ndicts = 0
    while True:
        if ndicts == ncpu:
            break
        if output_queue.empty():
            continue
        _tmp = output_queue.get()
        if type(_tmp) == str:
            print _tmp
        else:
            ndicts += 1
            for class_ in classes:
                for w, c in _tmp[class_].items():
                    total_ngram_count[w] += c
                    per_class_ngram_count[class_][w] += c

    for w in workers:
        w.join()

    for prefix, ngram_count in [('total', total_ngram_count)]+[('class%d' % idx, per_class_ngram_count[idx]) for idx in classes]:
        with gzip.open('%s.csv.gz' % prefix, 'wb') as ngramfile:
            ngramfile.write('ngram,count\n')
            for w, c in sorted(ngram_count.items(), key=lambda x: x[1]):
                ngramfile.write('%s,%s\n' % (w, c))
